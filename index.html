<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>ReverseLearning by abeoliver</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">ReverseLearning</h1>
      <h2 class="project-tagline">Science Fair project 2017 for Jadan Ercoli and Abraham</h2>
      <a href="https://github.com/abeoliver/ReverseLearning" class="btn">View on GitHub</a>
      <a href="https://github.com/abeoliver/ReverseLearning/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/abeoliver/ReverseLearning/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h3>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Input Optimization Algorithm</h3>

<p>A layer for Stochastic Gradient Descent for finding inputs within an input space that produce desired results in a machine learning model or mathematical function.</p>

<h3>
<a id="designer-templates" class="anchor" href="#designer-templates" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Abstract</h3>

<p>Statistics and artificial intelligence have long been concerned with summarizing data and predicting the future based on the past. These two fields have shown to be very effective in many fields such as natural language processing, financial predicting, and image recognition and have revolutionized the way in which the world works. However, what if an individual didn’t want to only predict their future, but to shape it? Instead of analyzing his or her current situation and predicting his or her future state, they wish to perform actions now that get them to a desired future. As a solution, we propose the Input Optimization Algorithm (IOA). The IOA finds and refines inputs that produces desired outputs. Imagine that a pharmaceuticals company  has a machine learning model that predicts the effectiveness of a new drug based on a patient’s weight, age, and dosage, which is not a simple function. Of course, any user would desire 100% effectiveness. With normal machine learning methods, the doctors would have to guess a dosage, check the predicted effectiveness, and use trial-and-error to find the dosage with the highest effectiveness. IOA, however, more robustly solves this problem. The doctors can set fixed age and weight and let IOA find the dosage. After the run, IOA will produce the optimal dosage. IOA also functions on a narrowed solution space. Certain factors can be set to plausible ranges, so as to produce meaningful results. Instead of predicting the future, IOA perfects it.</p>

<h3>
<a id="creating-pages-manually" class="anchor" href="#creating-pages-manually" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Gradient Descent</h3>

<p>Gradient descent is a class of iterative algorithms that change desired parameters in the opposite (negative) direction of their gradients. The algorithm takes the partial derivative of each parameter with respect to a function and then either raises or decreases each parameter to minimize the function. Usually, this algorithm is used to change the weights, biases, or other parameters of machine learning models like artificial neural networks and linear regressions with respect to their loss functions, functions that describe the error of the model.
	As an example, imagine that a parabola is the graph of a loss function for a function of one parameter. The x-axis represents the value of the parameter and the y-axis is the loss with lower y-values signifying lower loss. Now imagine that we place a small ball randomly on the surface. If we imagine this system on earth, classical physics say that the ball will roll down the slope because of the pull of gravity. Similarly, gradient descent uses calculus to determine which direction is “down” and “rolls the ball” in that direction. Just like in Figure 1, the ball moves to a new point that now has a lower loss, and has “learned” better parameters. The algorithm repeats this step hundreds, thousands, or even millions of times until the ball is sitting at the very bottom, the minimum, where there is no more down, like in Figure 2. This algorithm functions the same way in systems of 3 or more variables. Figure 3, shows the loss function from a linear regression model, a function of two parameters.
	There are many algorithms built on top of gradient descent that were designed to combat certain challenges that are faced in many scenarios. The most used variation is Stochastic Gradient Descent (SGD) which is very true to the original algorithm but deals with training data in certain ways. Most other variations are built off of SGD. The Momentum Method for SGD which increases the “speed” of the changing parameters when the gradients are steeper to combat SGD’s downfalls with steep “ravines” in the loss function. The Adagrad variation adapts the learning rate to the frequency of a given parameter, making larger steps for for infrequent parameters and taking smaller steps for frequent parameters. The Adaptive Moment Estimation (Adam) variation adapts the learning rate (similar to Adagrad) as well as adapting the learning rates (like Momentum)..</p>

<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Authors and Contributors</h3>

<p>You can @mention a GitHub username to generate a link to their profile. The resulting <code>&lt;a&gt;</code> element will link to the contributor’s GitHub Profile. For example: In 2007, Chris Wanstrath (<a href="https://github.com/defunkt" class="user-mention">@defunkt</a>), PJ Hyett (<a href="https://github.com/pjhyett" class="user-mention">@pjhyett</a>), and Tom Preston-Werner (<a href="https://github.com/mojombo" class="user-mention">@mojombo</a>) founded GitHub.</p>

<h3>
<a id="support-or-contact" class="anchor" href="#support-or-contact" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Support or Contact</h3>

<p>Having trouble with Pages? Check out our <a href="https://help.github.com/pages">documentation</a> or <a href="https://github.com/contact">contact support</a> and we’ll help you sort it out.</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/abeoliver/ReverseLearning">ReverseLearning</a> is maintained by <a href="https://github.com/abeoliver">abeoliver</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
